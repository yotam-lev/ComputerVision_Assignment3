\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage[pagebackref=false,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{****} 

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Efficient Gesture Recognition with Temporal Shift Modules on the Jester Dataset}

\author{Yotam Lev\\
Leiden University\\
s4045513
\and
Piotr Perski\\
Leiden University\\
s4102533
}

\maketitle

\begin{abstract}
      Gesture recognition is a fundamental problem in computer vision with applications ranging from human-computer interaction to sign language interpretation. In this work, we address the challenge of recognizing 27 hand gestures from the Jester dataset under the constraint of efficient temporal modeling and a training budget of under 2 hours. We implement and compare two approaches: a 2D CNN baseline (ResNet18) that processes single frames, and a Temporal Shift Module (TSM) network that efficiently models temporal dynamics by shifting features along the temporal dimension. Our experiments demonstrate that while the 2D baseline struggles to capture motion-dependent gestures, the TSM approach significantly improves performance without adding computational overhead. We achieve a final validation accuracy of 85\% with our best model, demonstrating the effectiveness of temporal shifting for gesture recognition tasks. Our code is available at \url{https://github.com/yotam-lev/ComputerVision_Assignment3}.

\end{abstract}

\section{Introduction}

Hand gesture recognition is a critical component of modern human-computer interaction systems. Unlike static image classification, gesture recognition requires understanding both the spatial appearance of the hand and its temporal motion. For example, the gestures "Swiping Left" and "Swiping Right" may look identical in a single frame but are distinguished entirely by their motion over time.

The Jester dataset \cite{jester} provides a challenging benchmark for this task, containing 27 distinct gesture classes. A key challenge in deploying such models is balancing accuracy with computational efficiency. 3D Convolutional Neural Networks (3D CNNs) are powerful for spatiotemporal modeling but are computationally expensive and difficult to train. Conversely, 2D CNNs are efficient but lack the ability to model temporal context explicitly.

In this project, we operate under the constraint of \textbf{maximizing accuracy with efficient temporal modeling}, with a strict goal of completing training in under 2 hours. We aim to build a model that outperforms a simple 2D baseline by incorporating temporal information without the heavy computational cost associated with 3D convolutions. To achieve this, we employ the Temporal Shift Module (TSM) \cite{tsm}, which shifts feature maps along the temporal dimension to facilitate information exchange between neighboring frames.


Our contributions are as follows:
\begin{itemize}
    \item We implement a 2D ResNet18 baseline to establish a performance floor.
    \item We implement a TSM-ResNet18 model to capture temporal dynamics efficiently.
    \item We evaluate both models on the Jester validation set, achieving 85\% accuracy with TSM, and analyze the results quantitatively and qualitatively.
\end{itemize}

\section{Related Work}

\textbf{2D CNNs for Video.} Early approaches to video classification applied 2D CNNs to individual frames and aggregated predictions using averaging or Recurrent Neural Networks (RNNs) like LSTMs \cite{donahue2015long}. While efficient, single-frame 2D CNNs fail to capture short-term temporal dependencies essential for distinguishing motion-based gestures.

\textbf{3D CNNs.} To model spatiotemporal features directly, 3D CNNs (e.g., C3D \cite{tran2015learning}, I3D \cite{carreira2017quo}) extend convolutions to the temporal dimension. While effective, they introduce a significant increase in parameters and computational cost (FLOPs), making them harder to train and deploy on resource-constrained devices.

\textbf{Temporal Shift Module (TSM).} Lin et al. \cite{tsm} proposed TSM as a primitive to enable 2D CNNs to learn temporal features. By shifting a portion of the channels along the temporal dimension, TSM allows information from past and future frames to mix with the current frame's features. This operation is parameter-free and FLOP-free, offering the performance of 3D CNNs with the complexity of 2D CNNs.

\section{Methodology}

\subsection{Dataset and Preprocessing}
We use the Jester dataset, consisting of video clips of 27 hand gestures. We utilized the provided subset containing 20\% of the training data (approx. 23k videos) and the full validation set (approx. 14k videos).

\textbf{Preprocessing.} Videos are decoded into frames. During training, we apply data augmentation to prevent overfitting:
\begin{itemize}
    \item \textbf{Spatial:} Random cropping to $224 \times 224$.
    \item \textbf{Normalization:} Images are normalized using ImageNet mean and standard deviation.
\end{itemize}
During validation, we resize frames to 256 pixels and take a center crop of $224 \times 224$.

\subsection{Baseline Model: 2D ResNet18}
Our baseline model is a standard ResNet18 \cite{he2016deep} pre-trained on ImageNet.
\begin{itemize}
    \item \textbf{Input:} A single randomly sampled frame from the video clip.
    \item \textbf{Architecture:} We replace the final fully connected layer to output 27 classes.
    \item \textbf{Training:} The model treats each frame as an independent image. During inference, we sample a single frame to predict the video label.
\end{itemize}
This baseline serves to measure how much performance can be achieved purely from spatial cues (e.g., hand pose) without explicit motion information.

\subsection{Improved Model: TSM ResNet18}
To address the lack of temporal modeling in the baseline, we implement a TSM-equipped ResNet18.
\begin{itemize}
    \item \textbf{Input:} A clip of $N=12$ frames sampled uniformly from the video.
    \item \textbf{Temporal Shift:} We insert TSM blocks into the residual stages of ResNet18. The TSM operation shifts 1/8 of the channels backward and 1/8 forward along the temporal dimension. This allows the convolution operations in subsequent layers to process features from adjacent frames, effectively creating a temporal receptive field.
    \item \textbf{Consensus:} The model outputs predictions for each of the 12 frames. We apply a consensus function (averaging) to obtain the final video-level prediction.
\end{itemize}

\subsection{Training Details}
Both models were implemented in PyTorch. We used the Cross-Entropy loss function and the Stochastic Gradient Descent (SGD) optimizer with momentum 0.9 and weight decay $5 \times 10^{-4}$. To ensure fair comparison and reproducibility, we used the same random seed for both experiments.
\begin{itemize}
    \item \textbf{Batch Size:} 16
    \item \textbf{Learning Rate:} Initialized at 0.001, decayed by a factor of 0.1 every 7 epochs.
    \item \textbf{Epochs:} Models were trained for up to 20 epochs.
\end{itemize}

\textbf{Training Time and Hardware.}
Training duration varied significantly depending on the model complexity and hardware used.
\begin{itemize}
    \item \textbf{Baseline:} $\sim$30 minutes on a MacBook Pro M3 (18GB RAM).
    \item \textbf{TSM:} Slightly over 2 hours on lab workstations ($\sim$6.8GB VRAM).
    \item \textbf{TIPS:} $\sim$2 hours and 15 minutes on the same lab hardware.
\end{itemize}

\section{Experiments and Results}

\subsection{Quantitative Results}

We evaluated both models on the full Jester validation set. Table \ref{tab:results} summarizes the results.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Input} & \textbf{Val Accuracy} \\
\midrule
Baseline (2D ResNet18) & 1 Frame & 55.0\% \\
\textbf{TSM ResNet18 (Ours)} & \textbf{12 Frames} & \textbf{85.0\%} \\
\bottomrule
\end{tabular}
\caption{Comparison of validation accuracy between the single-frame baseline and the TSM model. The TSM model significantly outperforms the baseline.}
\label{tab:results}
\end{table}

The baseline model achieves 55\% accuracy. While it can recognize static gestures (e.g., "Thumb Up"), it fails significantly on dynamic gestures where motion direction is key.

Our TSM model achieves a validation accuracy of \textbf{85\%}, with a training accuracy of \textbf{92\%}. This substantial improvement confirms that efficient temporal modeling is crucial for this dataset. The gap between training and validation accuracy suggests some overfitting, which is expected given we used only a 20\% subset of the training data.

\subsection{Qualitative Analysis}

To understand the model's behavior, we analyzed the confusion matrix (Figure \ref{fig:confusion}) to identify specific success and failure cases.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{Confusion_Matrix.png}
\caption{Confusion Matrix for the TSM model (12 frames). The strong diagonal indicates high accuracy, while off-diagonal clusters reveal specific confusion patterns.}
\label{fig:confusion}
\end{figure}

\textbf{Overall Performance.} The most obvious feature is the strong dark blue diagonal, confirming our quantitative result of 85\% accuracy. For most classes, the model correctly predicts the true label the vast majority of the time (e.g., "Thumb Up" has 507 correct predictions vs. negligible errors). This validates that the TSM module is successfully capturing the primary spatiotemporal features for distinct gestures.

\textbf{Major Failure Modes.} The off-diagonal numbers reveal three specific types of confusion where the model consistently fails:

\textit{1. Rotational Ambiguity.} The most significant confusion occurs between gestures that are "mirror images" of each other in time or direction. For instance, "Rolling Hand Forward" is misclassified as "Rolling Hand Backward" 102 times, and vice versa (86 times). Similarly, "Turning Hand Clockwise" is mistaken for "Counterclockwise" 85 times. These gestures look spatially identical in any single frame; they only differ by the temporal order. The high confusion suggests that while TSM captures linear motion well, it struggles with complex rotational dynamics or the temporal resolution (12 frames) isn't high enough to capture the subtle phase of the rotation.

\textit{2. Static Pose vs. Dynamic Motion.} There are 64 instances where a "Stop Sign" is misclassified as "Pushing Hand Away". Both gestures involve an open palm facing the camera. "Stop Sign" is static, while "Pushing" involves motion. The model likely focuses too much on the strong spatial feature (the open palm) and misses the "lack of motion" cue.

\textit{3. Fine-Grained Hand Shape.} "Sliding Two Fingers Right" is misclassified as "Swiping Right" 46 times. The motion is identical, but the hand shape differs (2 fingers vs. 5 fingers). The model correctly identifies the motion direction but fails to distinguish the subtle difference in hand shape, likely due to the spatial downsampling in ResNet ($224 \times 224$ input).

\textbf{Summary.} While the TSM model achieves high accuracy on distinct linear gestures (e.g., Swiping Left/Right), the confusion matrix reveals specific limitations. The highest error rates occur in reciprocal rotational gestures, with "Rolling Hand Forward" frequently confused with "Rolling Hand Backward". Additionally, semantic confusion persists between gestures with identical motion but different hand shapes (e.g., "Sliding Two Fingers" vs. "Swiping"), suggesting that the model prioritizes dominant motion cues over fine-grained spatial details.

\section{Discussion}

Our constraint was to achieve high accuracy with efficient temporal modeling. The TSM approach satisfies this by adding zero FLOPs to the backbone network while enabling strong temporal reasoning.
The baseline's poor performance highlights that spatial features alone are insufficient for gesture recognition. By simply shifting channels, TSM transforms a standard 2D CNN into a pseudo-3D CNN capable of understanding "before" and "after" relationships.

One limitation of our approach is the use of a small subset (20\%) of the training data. With the full dataset, we expect the model would generalize better and potentially reach higher accuracy (>90\%). Furthermore, increasing the number of frames from 12 to 16 or 24 could improve recognition of longer, more complex gestures, albeit at a higher memory cost.

We also explored Translation Invariant Polyphase Sampling (TIPS) \cite{saha2025improving}, which is designed to improve robustness to spatial shiftsâ€”a common issue in gesture recognition where the hand position varies. We hypothesized that TIPS would improve accuracy over our TSM model by better handling these spatiotemporal shifts. However, in our experiments, this approach achieved a validation accuracy of 80\%, failing to outperform the TSM model (85\%). Since this method did not yield better results, we include it here only as a comparative experiment. We attribute this lack of improvement to the small training sample size (20\% subset), which may have been insufficient for the model to effectively learn the learnable pooling operators in TIPS.

\section{Conclusion}

In this assignment, we developed a gesture recognition pipeline using the Jester dataset. We demonstrated that a single-frame 2D CNN is inadequate for capturing the temporal dynamics of hand gestures. By implementing a Temporal Shift Module (TSM), we achieved a significant performance boost, reaching 85\% validation accuracy. This result validates TSM as a highly efficient method for spatiotemporal modeling, meeting our design goal of high performance with low computational overhead.

\section{Individual Contributions}

\begin{itemize}
    \item \textbf{Yotam Lev:} Implemented the data loading pipeline (`dataset.py`), the TSM model architecture (`tsm.py`), and managed the training experiments on the GPU.
    \item \textbf{Piotr Perski:} Implemented the baseline model (`baseline.py`), the evaluation script (`evaluate.py`), conducted the qualitative analysis, and wrote the report.
\end{itemize}
Both authors contributed equally to the overall design and debugging of the pipeline.

{\small
\bibliographystyle{ieee}
\begin{thebibliography}{1}

\bibitem{jester}
TwentyBN.
\newblock ``The 20bn-jester Dataset v1,''
\newblock [Online]. Available: \url{https://20bn.com/datasets/jester}.

\bibitem{tsm}
J.~Lin, C.~Gan, and S.~Han.
\newblock ``TSM: Temporal Shift Module for Efficient Video Understanding,''
\newblock in \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2019, pp. 7083--7093.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock ``Deep Residual Learning for Image Recognition,''
\newblock in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2016, pp. 770--778.

\bibitem{donahue2015long}
J.~Donahue, L.~A. Hendricks, S.~Guadarrama, M.~Rohrbach, S.~Venugopalan, K.~Saenko, and T.~Darrell.
\newblock ``Long-term Recurrent Convolutional Networks for Visual Recognition and Description,''
\newblock in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2015, pp. 2625--2634.

\bibitem{tran2015learning}
D.~Tran, L.~Bourdev, R.~Fergus, L.~Torresani, and M.~Paluri.
\newblock ``Learning Spatiotemporal Features with 3D Convolutional Networks,''
\newblock in \textit{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}, 2015, pp. 4489--4497.

\bibitem{carreira2017quo}
J.~Carreira and A.~Zisserman.
\newblock ``Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset,''
\newblock in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2017, pp. 6299--6308.

\bibitem{saha2025improving}
S.~Saha and T.~Gokhale.
\newblock ``Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling,''
\newblock in \textit{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 2025, pp. 620--629.

\end{thebibliography}
}

\end{document}
